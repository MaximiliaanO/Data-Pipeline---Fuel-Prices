
# â›½ Data Pipeline: Fuel Price
Automated web scraper that collects fuel prices from a website, transforms the data and loads the data in a **PostgreSQL database** for further analysis.  
This project runs daily via a cronjob and keeps historical price data by writing each scrape event to a fact table.

## ğŸ“Œ Features

- Scrapes **all gas stations** listed on the operator's website
- Extracts embedded JSON containing station metadata  
- Visits each station page individually to collect fuel prices  
- Automatically updates:
  - `dim_stations` (station metadata)
  - `fact_prices` (daily event-based fuel prices)
- Logs results to daily rotating log files  
- Fully configurable via `.env`  
- Ready for automated scheduling with cron  
- Modular Python structure:
  - `scraper.py` â†’ web requests, HTML parsing, price extraction
  - `db.py` â†’ PostgreSQL handling, inserts, updates
  - `main.py` â†’ orchestrates full ETL pipeline

---

## ğŸ—‚ Project Structure


```
Fuel_Scraper
â”œâ”€ env
|   â””â”€ .env
â”œâ”€ README.md
â”œâ”€ scripts
â”‚  â”œâ”€ db.py
â”‚  â”œâ”€ scraper.py
â”‚  â”œâ”€ sql
â”‚  â”‚  â”œâ”€ insertdim.sql
â”‚  â”‚  â”œâ”€ insertfact.sql
â”‚  â”‚  â”œâ”€ latest_event_id.sql
â”‚  â”‚  â””â”€ tables.sql
â”‚  â””â”€ __init__.py
â”œâ”€ tests
â”‚  â”œâ”€ conftest.py
â”‚  â”œâ”€ test_db.py
â”‚  â”œâ”€ test_main.py
â”‚  â”œâ”€ test_scraper.py
â”‚  â””â”€ __init__.py
â”œâ”€ __init__.py
â””â”€ main.py

```

---

## âš™ï¸ Installation

### 1. Clone repository

```bash
git clone https://github.com/<yourusername>/fuel-scraper.git
cd fuel-scraper
````

### 2. Install Python dependencies

Make sure youâ€™re using Python 3.10+.

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables (.env)

Copy env/.env.example to env/.env and fill in your values:

```
env/.env
```

With:

```
DB_NAME=...
DB_USER=...
DB_PASS=...
DB_HOST=...

BASE_URL=https://example.com
USER_AGENT=FuelScraper/1.0
```

Environment variables are loaded inside:

* `scraper.py` (BASE_URL, USER_AGENT)
* `db.py` (database connection)

---

## ğŸ Running the Scraper

```bash
python main.py
```

The workflow performed:

1. Download main webpage (`download_html`)
2. Extract station list JSON (`parse_html_stations`)
3. Create DB tables if needed (`create_db_tables`)
4. Transform and Insert/Update station metadata (`update_station_data`)
5. Fetch latest event ID from Database (`latest_event_id`)
6. Scrape individual station prices (`retrieve_individual_prices`)
7. Transform and insert fact price data (`update_fact_data`)
8. Commit and close connection

---

## ğŸ“ Logging

All scraper components write logs into the `logs/` directory.

Example configuration from code:

* `main.py` â†’ `logs/main.log`
* `scraper.py` â†’ `logs/scraper.log`
* `db.py` â†’ `logs/db.log`

These logs record:

* HTTP activity  (INFO - Disabled by default)
* SQL operations (INFO - Disabled by default)
* Missing price data (WARNING)
* Errors and warnings (WARNING)

Daily log rotation can optionally be added using
`TimedRotatingFileHandler`.

---

## ğŸ§© Database Schema

### Dimension Table: `dim_stations`

Stores metadata for every station:

| Column        | Type  | Description          |
| ------------- | ----- | -------------------- |
| id            | int   | Station ID           |
| brand         | text  | Station brand        |
| guid          | text  | Link to detail page  |
| title         | text  | Name                 |
| street        | text  | Address              |
| postcode      | text  | Cleaned postcode     |
| city          | text  | City                 |
| category      | jsonb | Station category     |
| lat           | float | Latitude             |
| long          | float | Longitude            |
| open_time     | text  | Open / not open      |
| gasolineTypes | jsonb | Available fuel types |
| services      | jsonb | Additional services  |

### Fact Table: `fact_prices`

Stores event-based fuel prices:

| Column     | Type     |
| ---------- | -------- |
| event_id   | int - PK |
| station_id | int - FK |
| timestamp  | datetime |
| fuel_type  | text     |
| price      | float    |

The scraper inserts **one event per station** with available fuel prices.

---

## â± Schedule Daily via Cron (Ubuntu)

Edit crontab:

```bash
crontab -e
```

Add:

```bash
0 8 * * * /usr/bin/python3 /home/username/fuel-scraper/main.py >> /home/username/fuel-scraper/logs/cron.log 2>&1
```

This runs once per day at 08:00.

---

## ğŸ§ª Testing (Unit Tests)

Unit tests use:

* `pytest`
* `unittest.mock` for mocking:

  * HTTP requests
  * DB cursors
  * BeautifulSoup parsing

Example:

```bash
pytest -v
```

---

## ğŸš¨ Error Handling

* SQL errors â†’ rollback + continue
* HTML missing price â†’ logged + skipped
* Missing fields in JSON â†’ logged
* Connection errors â†’ exception thrown

The scraper is highly fault-tolerant and continues processing stations even if a subset fails.

---

## ğŸ”® Potential Future Improvements

* Async scraping (1000+ station requests â†’ few minutes)
* Progress bar for scraping
* Docker container for easy deployment
* Power BI dashboard template (visualisation)
* Notifications on price changes

---

## ğŸ“„ License

MIT License (of Choice)

---

## ğŸ‘¤ Author

Max â€” Python Developer & Data Professional
